{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,GradientBoostingClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import pylab as plot\n",
    "params = { \n",
    "    'axes.labelsize': \"large\",\n",
    "    'xtick.labelsize': 'x-large',\n",
    "    'legend.fontsize': 20,\n",
    "    'figure.dpi': 150,\n",
    "    'figure.figsize': [10, 6]\n",
    "}\n",
    "plot.rcParams.update(params)\n",
    "\n",
    "PATH = \"./Kaggle/Titanic/data/\"\n",
    "train_raw=pd.read_csv(f'{PATH}train.csv',low_memory=False)\n",
    "test_raw=pd.read_csv(f'{PATH}test.csv',low_memory=False)\n",
    "\n",
    "train_raw.shape, test_raw.shape\n",
    "train_raw.head()\n",
    "data.describe()\n",
    "\n",
    "data = train_raw\n",
    "data['Died']= 1 - data['Survived']\n",
    "\n",
    "data.groupby('Sex').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True)\n",
    "sns.violinplot(x='Sex', y='Age', hue='Survived',data=data,split=True)\n",
    "\n",
    "figure = plt.figure(figsize=(32,16))\n",
    "plt.hist([data[data['Survived'] == 1]['Fare'], data[data['Survived'] == 0]['Fare']], \n",
    "         stacked=True,\n",
    "         bins = 50, label = ['Survived','Dead'])\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.legend();\n",
    "\n",
    "plt.figure(figsize=(25, 7))\n",
    "ax = plt.subplot()\n",
    "\n",
    "ax.scatter(data[data['Survived'] == 1]['Age'], data[data['Survived'] == 1]['Fare'], \n",
    "           c='green', s=data[data['Survived'] == 1]['Fare'])\n",
    "ax.scatter(data[data['Survived'] == 0]['Age'], data[data['Survived'] == 0]['Fare'], \n",
    "           c='red', s=data[data['Survived'] == 0]['Fare']);\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_ylabel('Average fare')\n",
    "data.groupby('Pclass').mean()['Fare'].plot(kind='bar',ax=ax)\n",
    "\n",
    "x_train = train_raw.drop(['Survived'],1)\n",
    "y_train = train_raw['Survived']\n",
    "x_test = test_raw\n",
    "\n",
    "df_combined = x_train.append(x_test)\n",
    "df_combined.shape\n",
    "\n",
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)\n",
    "        \n",
    "display_all(df_combined.tail().T)\n",
    "\n",
    "train_cats(df_combined)\n",
    "\n",
    "display_all(df_combined.isnull().sum().sort_index()/len(df_combined))\n",
    "df,y,nas = proc_df(df_combined,y_fld=None,ignore_flds=['Age','Name','Embarked','Cabin','Parch',\n",
    "                                                      'SibSp'])\n",
    "df.head()\n",
    "\n",
    "def process_family():\n",
    "    \n",
    "    global df\n",
    "    # introducing a new feature : the size of families (including the passenger)\n",
    "    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n",
    "    \n",
    "    # introducing other features based on the family size\n",
    "    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n",
    "    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5 <= s else 0)    \n",
    "    return df\n",
    "\n",
    "df = process_family()\n",
    "\n",
    "def process_embarked():\n",
    "    global df\n",
    "    # two missing embarked values - filling them with the most frequent one in the train  set(S)\n",
    "    df.Embarked.fillna('S', inplace=True)\n",
    "    # dummy encoding \n",
    "    df_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
    "    df = pd.concat([df, df_dummies], axis=1)\n",
    "    df.drop('Embarked', axis=1, inplace=True)\n",
    "#     status('embarked')\n",
    "    return df\n",
    "\n",
    "df = process_embarked()\n",
    "\n",
    "    global df    \n",
    "    # replacing missing cabins with U (for Uknown)\n",
    "    df.Cabin.fillna('T', inplace=True)\n",
    "    \n",
    "    # mapping each Cabin value with the cabin letter\n",
    "    df['Cabin'] = df['Cabin'].map(lambda c: c[0])\n",
    "    \n",
    "    # dummy encoding ...\n",
    "    cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')    \n",
    "    df = pd.concat([df, cabin_dummies], axis=1)\n",
    "\n",
    "    df.drop('Cabin', axis=1, inplace=True)\n",
    "#     status('cabin')\n",
    "    return df\n",
    "\n",
    "df = process_cabin()\n",
    "titles = set()\n",
    "for name in df['Name']:\n",
    "    titles.add(name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "Title_Dictionary = {\n",
    "    \"Capt\": \"Officer\",\n",
    "    \"Col\": \"Officer\",\n",
    "    \"Major\": \"Officer\",\n",
    "    \"Jonkheer\": \"Royalty\",\n",
    "    \"Don\": \"Royalty\",\n",
    "    \"Sir\" : \"Royalty\",\n",
    "    \"Dr\": \"Officer\",\n",
    "    \"Rev\": \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Mrs\",\n",
    "    \"Mr\" : \"Mr\",\n",
    "    \"Mrs\" : \"Mrs\",\n",
    "    \"Miss\" : \"Miss\",\n",
    "    \"Master\" : \"Master\",\n",
    "    \"Lady\" : \"Royalty\"\n",
    "}\n",
    "\n",
    "def get_titles():\n",
    "    # we extract the title from each name\n",
    "    df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
    "    \n",
    "    # a map of more aggregated title\n",
    "    # we map each title\n",
    "    df['Title'] = df.Title.map(Title_Dictionary)\n",
    "#     status('Title')\n",
    "    return df\n",
    "\n",
    "df = get_titles()\n",
    "df.head()\n",
    "\n",
    "grouped_train = df.groupby(['Sex','Pclass','Title'])\n",
    "grouped_median_train = grouped_train.median()\n",
    "grouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n",
    "\n",
    "grouped_median_train.head()\n",
    "\n",
    "def fill_age(row):\n",
    "    condition = (\n",
    "        (grouped_median_train['Sex'] == row['Sex']) & \n",
    "        (grouped_median_train['Title'] == row['Title']) & \n",
    "        (grouped_median_train['Pclass'] == row['Pclass'])\n",
    "    ) \n",
    "    if np.isnan(grouped_median_train[condition]['Age'].values[0]):\n",
    "        print('true')\n",
    "        condition = (\n",
    "            (grouped_median_train['Sex'] == row['Sex']) & \n",
    "            (grouped_median_train['Pclass'] == row['Pclass'])\n",
    "        )\n",
    "\n",
    "    return grouped_median_train[condition]['Age'].values[0]\n",
    "\n",
    "\n",
    "def process_age():\n",
    "    global df\n",
    "    # a function that fills the missing values of the Age variable\n",
    "    df['Age'] = df.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n",
    "#     status('age')\n",
    "    return df\n",
    "\n",
    "df = process_age()\n",
    "\n",
    "display_all(df.isnull().sum().sort_index()/len(df))\n",
    "df[df.Title.isnull()]\n",
    "\n",
    "def process_names():\n",
    "    global df\n",
    "    # we clean the Name variable\n",
    "    df.drop('Name', axis=1, inplace=True)\n",
    "    \n",
    "    # encoding in dummy variable\n",
    "    titles_dummies = pd.get_dummies(df['Title'], prefix='Title')\n",
    "    df = pd.concat([df, titles_dummies], axis=1)\n",
    "    \n",
    "    # removing the title variable\n",
    "    df.drop('Title', axis=1, inplace=True)\n",
    "    \n",
    "#     status('names')\n",
    "    return df\n",
    "\n",
    "df = process_names()\n",
    "\n",
    "display_all(df.isnull().sum().sort_index()/len(df))\n",
    "\n",
    "x_train = df[:891].copy()\n",
    "x_test = df[891:].copy()\n",
    "x_train.shape,x_test.shape\n",
    "\n",
    "def split_vals(a,n): return a[:n], a[n:]\n",
    "valid_count =60\n",
    "n_trn = len(x_train)-valid_count\n",
    "x_train1, x_valid1 = split_vals(x_train, n_trn)\n",
    "y_train1, y_valid1 = split_vals(y_train, n_trn)\n",
    "\n",
    "m = RandomForestClassifier(n_estimators=180,min_samples_leaf=3,max_features=0.5,n_jobs=-1)\n",
    "m.fit(x_train1,y_train1)\n",
    "m.score(x_train1,y_train1)\n",
    "\n",
    "y_predict=m.predict(x_valid1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_valid1,y_predict)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_valid1,y_predict))\n",
    "\n",
    "print(confusion_matrix(y_valid1,y_predict))\n",
    "\n",
    "fi = rf_feat_importance(m, x_train1); fi[:10]\n",
    "\n",
    "def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n",
    "plot_fi(fi[:30]);\n",
    "\n",
    "to_keep = fi[fi.imp>0.01].cols; len(to_keep)\n",
    "to_keep\n",
    "\n",
    "x_train = x_train[to_keep]\n",
    "x_train\n",
    "\n",
    "m = RandomForestClassifier(n_estimators=200,min_samples_leaf=3,max_features=0.5,n_jobs=-1)\n",
    "m.fit(x_train,y_train)\n",
    "m.score(x_train,y_train)\n",
    "\n",
    "x_test = x_test[to_keep]\n",
    "output=m.predict(x_test).astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
